{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "! rm -rf nano-BERT\n",
    "! rm -rf space-model\n",
    "! git clone https://github.com/StepanTita/nano-BERT.git\n",
    "! git clone https://github.com/StepanTita/space-model.git"
   ],
   "metadata": {
    "id": "V4vJIZJeclwP",
    "outputId": "fdc87315-cb42-4d1c-d945-5ed08225a8c1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/content/nano-BERT')\n",
    "sys.path.append('/content/space-model')"
   ],
   "metadata": {
    "id": "LFX6x0OIc6uP"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-10-17T21:21:52.683897Z",
     "end_time": "2023-10-17T21:21:56.394657Z"
    },
    "id": "EzQpePJ4cXPG"
   },
   "source": [
    "import math\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from model import NanoBertForClassification\n",
    "from tokenizer import WordTokenizer\n",
    "\n",
    "from space_model.model import SpaceModelForClassification\n",
    "from space_model.loss import *"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-17T21:21:57.485837Z",
     "end_time": "2023-10-17T21:21:57.492835Z"
    },
    "id": "dPxAI9XscXPH"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "data = None\n",
    "with open('nano-BERT/data/imdb_train.json') as f:\n",
    "    data = [json.loads(l) for l in f.readlines()]"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-17T21:21:58.202759Z",
     "end_time": "2023-10-17T21:21:58.663667Z"
    },
    "id": "KQb635bAcXPH"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "vocab = set()\n",
    "for d in data:\n",
    "    vocab |= set([w.lower() for w in d['text']])"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-17T21:21:59.290429Z",
     "end_time": "2023-10-17T21:21:59.380681Z"
    },
    "id": "MnzxttcxcXPI"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "test_data = None\n",
    "with open('nano-BERT/data/imdb_test.json') as f:\n",
    "   test_data = [json.loads(l) for l in f.readlines()]"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-17T21:21:59.785847Z",
     "end_time": "2023-10-17T21:21:59.795588Z"
    },
    "id": "m5CMy4NmcXPI"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def encode_label(label):\n",
    "    if label == 'pos':\n",
    "        return 1\n",
    "    elif label == 'neg':\n",
    "        return 0\n",
    "    raise Exception(f'Unknown Label: {label}!')\n",
    "\n",
    "\n",
    "class IMDBDataloader:\n",
    "    def __init__(self, data, test_data, tokenizer, label_encoder, batch_size, val_frac=0.2):\n",
    "        train_data, val_data = train_test_split(data, shuffle=True, random_state=42, test_size=val_frac)\n",
    "\n",
    "        self.splits = {\n",
    "            'train': [d['text'] for d in train_data],\n",
    "            'test': [d['text'] for d in test_data],\n",
    "            'val': [d['text'] for d in val_data]\n",
    "        }\n",
    "\n",
    "        self.labels = {\n",
    "            'train': [d['label'] for d in train_data],\n",
    "            'test': [d['label'] for d in test_data],\n",
    "            'val': [d['label'] for d in val_data]\n",
    "        }\n",
    "\n",
    "        self.tokenized = {\n",
    "            'train': [tokenizer(record).unsqueeze(0) for record in\n",
    "                      tqdm(self.splits['train'], desc='Train Tokenization')],\n",
    "            'test': [tokenizer(record).unsqueeze(0) for record in tqdm(self.splits['test'], desc='Test Tokenization')],\n",
    "            'val': [tokenizer(record).unsqueeze(0) for record in tqdm(self.splits['val'], desc='Val Tokenization')],\n",
    "        }\n",
    "\n",
    "        self.encoded_labels = {\n",
    "            'train': [label_encoder(label) for label in tqdm(self.labels['train'], desc='Train Label Encoding')],\n",
    "            'test': [label_encoder(label) for label in tqdm(self.labels['test'], desc='Test Label Encoding')],\n",
    "            'val': [label_encoder(label) for label in tqdm(self.labels['val'], desc='Val Label Encoding')],\n",
    "        }\n",
    "\n",
    "        self.curr_batch = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.iterate_split = None\n",
    "\n",
    "    def peek(self, split):\n",
    "        return {\n",
    "            'input_ids': self.splits[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)],\n",
    "            'label_ids': self.labels[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)],\n",
    "        }\n",
    "\n",
    "    def take(self, split):\n",
    "        batch = self.splits[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)]\n",
    "        labels = self.labels[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)]\n",
    "        self.curr_batch += 1\n",
    "        return {\n",
    "            'input_ids': batch,\n",
    "            'label_ids': labels,\n",
    "        }\n",
    "\n",
    "    def peek_tokenized(self, split):\n",
    "        return {\n",
    "            'input_ids': torch.cat(\n",
    "                self.tokenized[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)],\n",
    "                dim=0),\n",
    "            'label_ids': torch.tensor(\n",
    "                self.encoded_labels[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)],\n",
    "                dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def peek_index_tokenized(self, index, split):\n",
    "        return {\n",
    "            'input_ids': torch.cat(\n",
    "                [self.tokenized[split][index]],\n",
    "                dim=0),\n",
    "            'label_ids': torch.tensor(\n",
    "                [self.encoded_labels[split][index]],\n",
    "                dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def peek_index(self, index, split):\n",
    "        return {\n",
    "            'input_ids': [self.splits[split][index]],\n",
    "            'label_ids': [self.labels[split][index]],\n",
    "        }\n",
    "\n",
    "    def take_tokenized(self, split):\n",
    "        batch = self.tokenized[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)]\n",
    "        labels = self.encoded_labels[split][self.batch_size * self.curr_batch:self.batch_size * (self.curr_batch + 1)]\n",
    "        self.curr_batch += 1\n",
    "        return {\n",
    "            'input_ids': torch.cat(batch, dim=0),\n",
    "            'label_ids': torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def get_split(self, split):\n",
    "        self.iterate_split = split\n",
    "        return self\n",
    "\n",
    "    def steps(self, split):\n",
    "        return len(self.tokenized[split]) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.reset()\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.batch_size * self.curr_batch < len(self.splits[self.iterate_split]):\n",
    "            return self.take_tokenized(self.iterate_split)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def reset(self):\n",
    "        self.curr_batch = 0"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-17T21:22:01.646787Z",
     "end_time": "2023-10-17T21:22:01.652524Z"
    },
    "id": "V1EiqscxcXPI"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 128\n",
    "LEARNING_RATE = 1e-2"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-17T21:22:18.368560Z",
     "end_time": "2023-10-17T21:22:18.371680Z"
    },
    "id": "5b6HjLMEcXPJ"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "tokenizer = WordTokenizer(vocab=vocab, max_seq_len=MAX_SEQ_LEN)\n",
    "tokenizer"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-17T21:26:23.403439Z",
     "end_time": "2023-10-17T21:26:23.407128Z"
    },
    "id": "7CdJl4UEcXPJ",
    "outputId": "054a3aaa-ae35-43c9-b60c-129776875d9a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "dataloader = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-17T21:26:24.941273Z",
     "end_time": "2023-10-17T21:26:37.278112Z"
    },
    "id": "iQMDcK01cXPK",
    "outputId": "24e34528-c0b9-4159-ae88-8083f5de840b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "bert = NanoBertForClassification(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    n_layers=1,\n",
    "    n_heads=1,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    n_classes=2\n",
    ").to(device)\n",
    "bert"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-17T21:26:39.490237Z",
     "end_time": "2023-10-17T21:26:39.504430Z"
    },
    "id": "7LtAcsA8cXPK",
    "outputId": "4e888f82-eff1-4c5f-ec01-6c2f7c9315e6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-17T21:26:40.211284Z",
     "end_time": "2023-10-17T21:26:40.220320Z"
    },
    "id": "H64pDk8AcXPK"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "count_parameters(bert)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-17T21:26:40.956953Z",
     "end_time": "2023-10-17T21:26:40.969226Z"
    },
    "id": "i-Bhh81ScXPK",
    "outputId": "fb8758a4-b8e1-4064-ee71-f3639a7d5122",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "optimizer = torch.optim.Adam(bert.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    print(f'Epoch: {i + 1}')\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "\n",
    "    bert.train()\n",
    "    for step, batch in enumerate(tqdm(dataloader.get_split('train'), total=dataloader.steps('train'))):\n",
    "        logits = bert(batch['input_ids'].to(device)) # (B, Seq_Len, 2)\n",
    "\n",
    "        probs = F.softmax(logits[:, 0, :], dim=-1).cpu()\n",
    "        pred = torch.argmax(probs, dim=-1) # (B)\n",
    "        train_preds += pred.detach().tolist()\n",
    "        train_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "        loss = F.cross_entropy(logits[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    bert.eval()\n",
    "    for step, batch in enumerate(tqdm(dataloader.get_split('val'), total=dataloader.steps('val'))):\n",
    "        logits = bert(batch['input_ids'].to(device))\n",
    "\n",
    "        probs = F.softmax(logits[:, 0, :], dim=-1).cpu()\n",
    "        pred = torch.argmax(probs, dim=-1) # (B)\n",
    "        val_preds += pred.detach().tolist()\n",
    "        val_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "        loss = F.cross_entropy(logits[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    print()\n",
    "    print(f'Train loss: {train_loss / dataloader.steps(\"train\")} | Val loss: {val_loss / dataloader.steps(\"val\")}')\n",
    "    print(f'Train acc: {accuracy_score(train_labels, train_preds)} | Val acc: {accuracy_score(val_labels, val_preds)}')\n",
    "    print(f'Train f1: {f1_score(train_labels, train_preds)} | Val f1: {f1_score(val_labels, val_preds)}')"
   ],
   "metadata": {
    "id": "dlLiYGbjcXPL",
    "outputId": "22d3a7ca-12e1-4fb5-b191-15ea6cdb01aa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_loss = 0.0\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "\n",
    "bert.eval()\n",
    "for step, batch in enumerate(tqdm(dataloader.get_split('test'), total=dataloader.steps('test'))):\n",
    "    logits = bert(batch['input_ids'].to(device))\n",
    "\n",
    "    probs = F.softmax(logits[:, 0, :], dim=-1).cpu()\n",
    "    pred = torch.argmax(probs, dim=-1) # (B)\n",
    "    test_preds += pred.detach().tolist()\n",
    "    test_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "    loss = F.cross_entropy(logits[:, 0, :].cpu(), batch['label_ids'])\n",
    "\n",
    "    test_loss += loss.item()\n",
    "\n",
    "print()\n",
    "print(f'Test loss: {test_loss / dataloader.steps(\"test\")}')\n",
    "print(f'Test acc: {accuracy_score(test_labels, test_preds)}')\n",
    "print(f'Test f1: {f1_score(test_labels, test_preds)}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FimwkDtWqzD",
    "outputId": "6d99af1c-8d79-48c5-8735-b5c651b031ac"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add Space Model"
   ],
   "metadata": {
    "id": "oo5_5917roF9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SpaceBertForClassification(torch.nn.Module):\n",
    "    def __init__(self, base_model, n_embed=3, n_latent=3, n_concept_spaces=2, fine_tune=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if fine_tune:\n",
    "            for p in base_model.parameters():\n",
    "                p.requires_grad_(False)\n",
    "        self.bert = base_model\n",
    "        self.space_model = SpaceModelForClassification(n_embed, n_latent, n_concept_spaces)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.bert(x)\n",
    "\n",
    "        out = self.space_model(embed)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "id": "fMY-1ahgrngT"
   },
   "execution_count": 96,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "space_bert = SpaceBertForClassification(bert.nano_bert).to(device)\n",
    "space_bert"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JqMCe2z9s29J",
    "outputId": "b456fea5-42df-4d9c-c974-3b9e3bcce823"
   },
   "execution_count": 97,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "count_parameters(space_bert)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3HlyHbazs8-w",
    "outputId": "035a3f29-56de-4b8b-c41a-94c4640361a2"
   },
   "execution_count": 98,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "NUM_OPTIM_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-2\n",
    "\n",
    "l1 = 1e-6\n",
    "l2 = 1e-6"
   ],
   "metadata": {
    "id": "J6b-bkkXtYXb"
   },
   "execution_count": 100,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(space_bert.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for i in range(NUM_OPTIM_EPOCHS):\n",
    "    print(f'Epoch: {i + 1}')\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "\n",
    "    bert.train()\n",
    "    for step, batch in enumerate(tqdm(dataloader.get_split('train'), total=dataloader.steps('train'))):\n",
    "        out = space_bert(batch['input_ids'].to(device)) # (B, 2)\n",
    "\n",
    "        logits = out.logits.cpu()\n",
    "        concept_spaces = [c.cpu() for c in out.concept_spaces]\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1).cpu()\n",
    "        pred = torch.argmax(probs, dim=-1) # (B)\n",
    "        train_preds += pred.detach().tolist()\n",
    "        train_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "        # CE + l1 * inter_loss + l2 * intra_loss\n",
    "        loss = F.cross_entropy(logits, batch['label_ids']) + l1 * inter_space_loss(concept_spaces) + l2 * intra_space_loss(concept_spaces)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    bert.eval()\n",
    "    for step, batch in enumerate(tqdm(dataloader.get_split('val'), total=dataloader.steps('val'))):\n",
    "        out = space_bert(batch['input_ids'].to(device)) # (B, 2)\n",
    "\n",
    "        logits = out.logits.cpu()\n",
    "        concept_spaces = [c.cpu() for c in out.concept_spaces]\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1).cpu()\n",
    "        pred = torch.argmax(probs, dim=-1) # (B)\n",
    "        val_preds += pred.detach().tolist()\n",
    "        val_labels += [l.item() for l in batch['label_ids']]\n",
    "\n",
    "        # CE + l1 * inter_loss + l2 * intra_loss\n",
    "        loss = F.cross_entropy(logits, batch['label_ids']) + l1 * inter_space_loss(concept_spaces) + l2 * intra_space_loss(concept_spaces)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    print()\n",
    "    print(f'Train loss: {train_loss / dataloader.steps(\"train\")} | Val loss: {val_loss / dataloader.steps(\"val\")}')\n",
    "    print(f'Train acc: {accuracy_score(train_labels, train_preds)} | Val acc: {accuracy_score(val_labels, val_preds)}')\n",
    "    print(f'Train f1: {f1_score(train_labels, train_preds)} | Val f1: {f1_score(val_labels, val_preds)}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3dWi-Frszp1",
    "outputId": "b80159bc-9821-486a-812e-cdffcf47dcf3"
   },
   "execution_count": 101,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Interpreting and visualizing the results"
   ],
   "metadata": {
    "id": "IlCBCY4xlUMj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_dataloader = IMDBDataloader(data, test_data, tokenizer, encode_label, batch_size=1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cssoF5oSXC2V",
    "outputId": "aa6cb19c-4cb2-4ee9-eb0e-55624a959b29"
   },
   "execution_count": 103,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# examples with less than 16 words are easier to visualize, so focus on them\n",
    "examples_ids = []\n",
    "for i, v in enumerate(test_dataloader.splits['test']):\n",
    "    if len(v) <= 16:\n",
    "        examples_ids.append(i)\n",
    "print(examples_ids)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRh1wc2Tf8Go",
    "outputId": "1e6b3191-ba2e-4500-9c0c-6f5a62ee55f3"
   },
   "execution_count": 104,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bert Embeddings"
   ],
   "metadata": {
    "id": "XBAlFqSWzo8h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "scatters = []\n",
    "for sample_index in examples_ids:\n",
    "    # extract example, decode to tokens and get the sequence length (ingoring padding)\n",
    "    test_tokenized_batch = test_dataloader.peek_index_tokenized(index=sample_index, split='test')\n",
    "    tokens = tokenizer.decode([t.item() for t in test_tokenized_batch['input_ids'][0] if t != 0], ignore_special=False).split(' ')[:MAX_SEQ_LEN]\n",
    "    seq_len = len(tokens)\n",
    "\n",
    "    embed = bert.nano_bert.embedding(test_tokenized_batch['input_ids'].to(device))\n",
    "\n",
    "    x, y, z = embed[0, :seq_len, 0].detach().cpu().numpy(), embed[0, :seq_len, 1].detach().cpu().numpy(), embed[0, :seq_len, 2].detach().cpu().numpy()\n",
    "\n",
    "    scatters.append(go.Scatter3d(\n",
    "        x=x, y=y, z=z, mode='markers+text', name=f'Example: {sample_index}',\n",
    "        text=tokens,\n",
    "    ))"
   ],
   "metadata": {
    "id": "mhA-lfKDlpSw"
   },
   "execution_count": 105,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fig = go.Figure(\n",
    "    data=scatters,\n",
    "    layout=go.Layout(\n",
    "        title=go.layout.Title(text='Embeddings')\n",
    "    ))\n",
    "fig.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "1G9cwASRfw5O",
    "outputId": "7861ba6e-3968-40ad-cc11-66f2d9335080"
   },
   "execution_count": 106,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "scatters = []\n",
    "for sample_index in examples_ids:\n",
    "    # extract example, decode to tokens and get the sequence length (ingoring padding)\n",
    "    test_tokenized_batch = test_dataloader.peek_index_tokenized(index=sample_index, split='test')\n",
    "    tokens = tokenizer.decode([t.item() for t in test_tokenized_batch['input_ids'][0] if t != 0], ignore_special=False).split(' ')[:MAX_SEQ_LEN]\n",
    "    seq_len = len(tokens)\n",
    "\n",
    "    embed = bert.nano_bert(test_tokenized_batch['input_ids'].to(device))\n",
    "\n",
    "    x, y, z = embed[0, :seq_len, 0].detach().cpu().numpy(), embed[0, :seq_len, 1].detach().cpu().numpy(), embed[0, :seq_len, 2].detach().cpu().numpy()\n",
    "\n",
    "    scatters.append(go.Scatter3d(\n",
    "        x=x, y=y, z=z, mode='markers+text', name=f'Example: {sample_index}',\n",
    "        text=tokens,\n",
    "    ))"
   ],
   "metadata": {
    "id": "ZrBYZVIhkixE"
   },
   "execution_count": 107,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fig = go.Figure(\n",
    "    data=scatters,\n",
    "    layout=go.Layout(\n",
    "        title=go.layout.Title(text='Raw Embeddings')\n",
    "    ))\n",
    "fig.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "fFbFqNyhnZPv",
    "outputId": "b1c0bbbf-bc95-4f4e-8c1e-42b1c9d1af1d"
   },
   "execution_count": 108,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Space Embeddings"
   ],
   "metadata": {
    "id": "E_GLqljEzrqQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "scatters = []\n",
    "colors = ['blue', 'red']\n",
    "for sample_index in examples_ids:\n",
    "    # extract example, decode to tokens and get the sequence length (ingoring padding)\n",
    "    test_tokenized_batch = test_dataloader.peek_index_tokenized(index=sample_index, split='test')\n",
    "    tokens = tokenizer.decode([t.item() for t in test_tokenized_batch['input_ids'][0] if t != 0], ignore_special=False).split(' ')[:MAX_SEQ_LEN]\n",
    "    seq_len = len(tokens)\n",
    "\n",
    "    bert_embed = space_bert.bert(test_tokenized_batch['input_ids'].to(device))\n",
    "\n",
    "    concept_spaces = space_bert.space_model.space_model(bert_embed).concept_spaces\n",
    "\n",
    "    for c, embed in enumerate(concept_spaces):\n",
    "        x, y, z = embed[0, :seq_len, 0].detach().cpu().numpy(), embed[0, :seq_len, 1].detach().cpu().numpy(), embed[0, :seq_len, 2].detach().cpu().numpy()\n",
    "\n",
    "        scatters.append(go.Scatter3d(\n",
    "            x=x, y=y, z=z, mode='markers+text',\n",
    "            name=f'Example: {sample_index} ({c})',\n",
    "            text=tokens,\n",
    "            marker=dict(color=colors[c]),\n",
    "            # hovertext=[]\n",
    "        ))"
   ],
   "metadata": {
    "id": "qBqm7w5Rncx6"
   },
   "execution_count": 122,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fig = go.Figure(\n",
    "    data=scatters,\n",
    "    layout=go.Layout(\n",
    "        title=go.layout.Title(text='Space Embeddings')\n",
    "    ))\n",
    "fig.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "6aDUDjh30Cti",
    "outputId": "453ebdf0-a831-449b-9775-9a4b08e4010b"
   },
   "execution_count": 123,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "3CfRqtTW0KyF"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
